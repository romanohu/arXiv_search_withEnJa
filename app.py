import streamlit as st
import datetime
import numpy as np
from sentence_transformers import util

# å¤–éƒ¨ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from utils import (
    get_wikipedia_related_words,
    extract_keywords,
    get_dynamic_related_words,
    fetch_arxiv_papers,
    encode_papers,
    get_model
)

# åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
model = get_model()

# ã‚¿ã‚¤ãƒˆãƒ«ã¨èª¬æ˜
st.title("arXivè«–æ–‡æ¤œç´¢")
with st.expander("ä½¿ã„æ–¹"):
    st.write("ã“ã‚Œã¯è‹±èªã§ã—ã‹æ¤œç´¢ã§ããªã„arXivã«å¯¾ã—ã¦æ—¥æœ¬èªã§ã®æ¤œç´¢ã‚’è©¦ã¿ã‚‹æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã§ã™ï¼ä»¥ä¸‹ã®æ‰‹é †ã§ä½¿ç”¨ã§ãã¾ã™:" \
    "\n" \
    "1. **arXivåˆ†é‡æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰**: arXivã§èª¿ã¹ãŸã„è«–æ–‡ãŒè©²å½“ã™ã‚‹å¤§ã¾ã‹ãªåˆ†é‡ã‚’ã€Œè‹±èªã€ã§å…¥åŠ›ã—ã¾ã™ï¼ä¾‹:Machine Learning, LLMãªã©ï¼" \
    "\n" \
    "2. **å–å¾—é–‹å§‹æ—¥ã¨çµ‚äº†æ—¥**: è«–æ–‡ã®å–å¾—æœŸé–“ã‚’æŒ‡å®šã—ã¾ã™ï¼" \
    "\n" \
    "3. **æ—¥æœ¬èªã‚¯ã‚¨ãƒª**: ã‚ˆã‚Šè©³ç´°ãªæ¤œç´¢ã—ãŸã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚„ãƒ•ãƒ¬ãƒ¼ã‚ºã‚’ã€Œæ—¥æœ¬èªã€ã§å…¥åŠ›ã—ã¾ã™ï¼" \
    "\n" \
    "4.   **å½¢æ…‹ç´ è§£æã®ä½¿ç”¨**: æ—¥æœ¬èªã‚¯ã‚¨ãƒªã‚’å½¢æ…‹ç´ è§£æã§åˆ†å‰²ã™ã‚‹ã‹ã©ã†ã‹ã‚’é¸æŠã—ã¾ã™ï¼" \
    "\n" \
    "5. **wikipediaã®è¨€èªé¸æŠ**: é–¢é€£èªã‚’æç¤ºã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã™ã‚‹wikipediaã®è¨€èªã‚’æŒ‡å®šã§ãã¾ã™" \
    "\n" \
    "6. **ç™ºè¡Œæ—¥ã®ä¸¦ã³é †**: è«–æ–‡ã®ç™ºè¡Œæ—¥ã®ä¸¦ã³é †ã‚’é¸æŠã—ã¾ã™ï¼" \
    "\n" \
    "7. **ä¸¦ã³æ›¿ãˆã®åŸºæº–**: æ¤œç´¢çµæœã®ä¸¦ã³æ›¿ãˆåŸºæº–ã‚’é¸æŠã—ã¾ã™ï¼" \
    "\n" \
    "8. **é–¢é€£èªã®é¸æŠ**: Wikipediaã‚„å½¢æ…‹ç´ è§£æã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸé–¢é€£èªã®å€™è£œã‹ã‚‰ï¼Œå¿…è¦ãªã‚‚ã®ã‚’é¸æŠã—ã¾ã™ï¼" \
    "\n" \
    "9. **æ¤œç´¢çµæœã®è¡¨ç¤º**: æ¤œç´¢çµæœãŒè¡¨ç¤ºã•ã‚Œã¾ã™ï¼å„è«–æ–‡ã®ã‚¿ã‚¤ãƒˆãƒ«ï¼Œè‘—è€…ï¼Œç™ºè¡Œæ—¥ï¼Œè¦ç´„ï¼Œé¡ä¼¼åº¦ã‚¹ã‚³ã‚¢ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ï¼" \
    "\n" \
    "10. **è‘—è€…ã§çµã‚Šè¾¼ã¿**: è‘—è€…åã§çµã‚Šè¾¼ã‚€ã“ã¨ãŒã§ãã¾ã™ï¼" \
    "\n" \
    "11. **è¦ç´„ã®è¡¨ç¤º**: å„è«–æ–‡ã®è¦ç´„ã‚’è¡¨ç¤ºã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼")   

# ãƒ¦ãƒ¼ã‚¶å…¥åŠ›: arXivåˆ†é‡
arxiv_keyword = st.text_input("arXivåˆ†é‡æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰(è‹±èª)")

# ãƒ¦ãƒ¼ã‚¶å…¥åŠ›: æœŸé–“é¸æŠ
col1, col2 = st.columns(2)
with col1:
    date_from = st.date_input("ğŸ“… å–å¾—é–‹å§‹æ—¥", value=datetime.date.today() - datetime.timedelta(days=30))
with col2:
    date_to = st.date_input("ğŸ“… å–å¾—çµ‚äº†æ—¥", value=datetime.date.today())

# æ—¥æœ¬èªã‚¯ã‚¨ãƒªã¨å½¢æ…‹ç´ è§£æã‚ªãƒ—ã‚·ãƒ§ãƒ³
query = st.text_input("æ—¥æœ¬èªã‚¯ã‚¨ãƒª")
use_morph = st.checkbox("æ—¥æœ¬èªã‚¯ã‚¨ãƒªã‚’å½¢æ…‹ç´ è§£æã§åˆ†å‰²ã™ã‚‹", value=True)

# Wikipediaä½¿ç”¨è¨€èªã®é¸æŠ
wiki_lang = st.radio("Wikipediaã®è¨€èª", options=["æ—¥æœ¬èª", "English"], index=0)
wiki_lang_code = "ja" if wiki_lang == "æ—¥æœ¬èª" else "en"

# ä¸¦ã³æ›¿ãˆè¨­å®š
sort_type = st.radio("ä¸¦ã³æ›¿ãˆã®åŸºæº–", options=["ã‚¹ã‚³ã‚¢é †", "ç™ºè¡Œæ—¥é †"], index=0)
sort_order = st.radio("ä¸¦ã³é †ã‚’é¸æŠã—ã¦ãã ã•ã„", options=["æ–°ã—ã„é †ï¼ˆé™é †ï¼‰", "å¤ã„é †ï¼ˆæ˜‡é †ï¼‰"], index=0)

# ä¸¦ã³æ›¿ãˆã«é–¢ã™ã‚‹è¨­å®šå¤‰æ•°ã‚’æ±ºå®š
if sort_type == "ã‚¹ã‚³ã‚¢é †":
    reverse_sort = "é™é †" in sort_order
    sort_key = "score"
else:
    reverse_sort = "é™é †" in sort_order
    sort_key = "published_dt"

# ã‚¯ã‚¨ãƒªã¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå…¥åŠ›ã•ã‚ŒãŸã¨ãã«å‡¦ç†ã‚’é–‹å§‹
if query and arxiv_keyword:
    # å½¢æ…‹ç´ è§£æã®ä½¿ç”¨æœ‰ç„¡ã«ã‚ˆã£ã¦ã‚¯ã‚¨ãƒªã‚’å‡¦ç†
    if use_morph:
        keywords = extract_keywords(query)
        keywords.append(query)  # å…ƒã®èªã‚‚åŠ ãˆã‚‹
    else:
        keywords = [query]

    # æŠ½å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¡¨ç¤º
    st.write("æŠ½å‡ºã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰:", ", ".join(keywords))

    base = query

    # Wikipediaã‹ã‚‰é–¢é€£èªã‚’å–å¾—
    candidates = get_wikipedia_related_words(base, lang=wiki_lang_code)
    if candidates:
        candidates = get_dynamic_related_words(base, candidates)

    # ãƒ¦ãƒ¼ã‚¶ã«ã‚ˆã‚‹é–¢é€£èªã®é¸æŠ
    selected = []
    if candidates:
        st.write("ğŸ’¡ é–¢é€£èªã®å€™è£œã‚’é¸ã‚“ã§ãã ã•ã„:")
        cols = st.columns(2)
        for i, word in enumerate(candidates):
            if cols[i % 2].checkbox(word):
                selected.append(word)

    # æ‹¡å¼µå¾Œã‚¯ã‚¨ãƒªã®ä½œæˆã¨è¡¨ç¤º
    final_query = " ".join(keywords + selected)
    st.markdown(f"**æ‹¡å¼µå¾Œã‚¯ã‚¨ãƒª:** `{final_query}`")

    # ã‚¯ã‚¨ãƒªãƒ™ã‚¯ãƒˆãƒ«ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
    query_vec = model.encode(final_query, convert_to_numpy=True)

    # è«–æ–‡ã®å–å¾—å‡¦ç†é–‹å§‹
    with st.spinner("arXivã‹ã‚‰è«–æ–‡å–å¾—ä¸­..."):
        papers = fetch_arxiv_papers(
            query=arxiv_keyword,
            max_results=1000,
            date_from=date_from,
            date_to=date_to
        )

        # è©²å½“ã™ã‚‹è«–æ–‡ãŒãªã„å ´åˆã®å‡¦ç†
        if not papers:
            st.warning("æŒ‡å®šã•ã‚ŒãŸæœŸé–“ã«è©²å½“ã™ã‚‹è«–æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼")
        else:
            st.success(f"âœ… {len(papers)}ä»¶ã®è«–æ–‡ã‚’å–å¾—ã—ã¾ã—ãŸ")

            # ç™ºè¡Œæ—¥ã‚’datetimeå‹ã«å¤‰æ›
            for p in papers:
                p["published_dt"] = datetime.datetime.strptime(p["published"], "%Y-%m-%d")

            # è‘—è€…ã§çµã‚Šè¾¼ã¿
            selected_authors = st.multiselect("ğŸ‘¤ è‘—è€…ã§çµã‚Šè¾¼ã¿", sorted(set(a for p in papers for a in p["authors"])))
            if selected_authors:
                papers = [p for p in papers if any(a in selected_authors for a in p["authors"])]

            # é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—
            with st.spinner("ğŸ”„ ã‚¹ã‚³ã‚¢è¨ˆç®—ä¸­..."):
                paper_vecs = encode_papers(papers)
                scores = util.cos_sim(query_vec, paper_vecs)[0].cpu().numpy()
                papers = [p | {"score": scores[i]} for i, p in enumerate(papers)]

            # ä¸¦ã³æ›¿ãˆã®å®Ÿè¡Œ
            if sort_key == "score":
                papers = sorted(papers, key=lambda x: x["score"], reverse=reverse_sort)
            else:
                papers = sorted(papers, key=lambda x: x["published_dt"], reverse=reverse_sort)

            # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³
            results_per_page = 10
            total_results = len(papers)
            total_pages = (total_results - 1) // results_per_page + 1
            selected_page = st.number_input("ğŸ“„ è¡¨ç¤ºãƒšãƒ¼ã‚¸ã‚’é¸ã‚“ã§ãã ã•ã„", 1, total_pages, 1)
            start_idx = (selected_page - 1) * results_per_page
            end_idx = start_idx + results_per_page

            # æ¤œç´¢çµæœã®è¡¨ç¤º
            st.subheader(f"ğŸ“„ æ¤œç´¢çµæœï¼ˆãƒšãƒ¼ã‚¸ {selected_page} / {total_pages}ï¼‰")
            for idx, p in enumerate(papers[start_idx:end_idx], start=start_idx + 1):
                st.write(f"{idx}. [{p['title']}]({p['link']})")
                st.write(f"**Authors**: {', '.join(p['authors'])}")
                st.write(f"**Published**: {p['published']}")
                st.write(f"**Similarity**: `{p['score']:.4f}`")
                # é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢ã«å¿œã˜ãŸé€²æ—ãƒãƒ¼
                progress_value = min(max(int(p['score'] * 100), 0), 100)
                st.progress(progress_value)
                with st.expander("ğŸ“– è¦ç´„ã‚’è¡¨ç¤º"):
                    st.write(p["summary"])
