import streamlit as st
import datetime
import numpy as np
from sentence_transformers import util

from utils import (
    get_wikipedia_related_words,
    extract_keywords,
    get_dynamic_related_words,
    fetch_arxiv_papers,
    encode_papers,
    get_model
)

model = get_model()

st.title("arXivè«–æ–‡æ¤œç´¢")
with st.expander("ä½¿ã„æ–¹"):
    st.write("ã“ã®ã‚¢ãƒ—ãƒªã¯ã€arXivã‹ã‚‰è«–æ–‡ã‚’æ¤œç´¢ã—ã€é–¢é€£ã™ã‚‹è«–æ–‡ã‚’è¦‹ã¤ã‘ã‚‹ãŸã‚ã®ã‚‚ã®ã§ã™ã€‚ä»¥ä¸‹ã®æ‰‹é †ã§ä½¿ç”¨ã§ãã¾ã™:" \
    "\n" \
    "1. **arXivåˆ†é‡æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰**: æ¤œç´¢ã—ãŸã„arXivã®åˆ†é‡ã‚’ã€Œè‹±èªã€ã§å…¥åŠ›ã—ã¾ã™ã€‚ä¾‹:Machine Learning, transformerãªã©ã€‚" \
    "\n" \
    "2. **æ—¥æœ¬èªã‚¯ã‚¨ãƒª**: æ¤œç´¢ã—ãŸã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚„ãƒ•ãƒ¬ãƒ¼ã‚ºã‚’æ—¥æœ¬èªã§å…¥åŠ›ã—ã¾ã™ã€‚" \
    "\n" \
    "3. **å½¢æ…‹ç´ è§£æã®ä½¿ç”¨**: æ—¥æœ¬èªã‚¯ã‚¨ãƒªã‚’å½¢æ…‹ç´ è§£æã§åˆ†å‰²ã™ã‚‹ã‹ã©ã†ã‹ã‚’é¸æŠã—ã¾ã™ã€‚" \
    "\n" \
    "4. **å–å¾—é–‹å§‹æ—¥ã¨çµ‚äº†æ—¥**: è«–æ–‡ã®å–å¾—æœŸé–“ã‚’æŒ‡å®šã—ã¾ã™ã€‚" \
    "\n" \
    "5. **ç™ºè¡Œæ—¥ã®ä¸¦ã³é †**: è«–æ–‡ã®ç™ºè¡Œæ—¥ã®ä¸¦ã³é †ã‚’é¸æŠã—ã¾ã™ã€‚" \
    "\n" \
    "6. **ä¸¦ã³æ›¿ãˆã®åŸºæº–**: æ¤œç´¢çµæœã®ä¸¦ã³æ›¿ãˆåŸºæº–ã‚’é¸æŠã—ã¾ã™ã€‚" \
    "\n" \
    "7. **é–¢é€£èªã®é¸æŠ**: Wikipediaã‚„å½¢æ…‹ç´ è§£æã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸé–¢é€£èªã®å€™è£œã‹ã‚‰ã€å¿…è¦ãªã‚‚ã®ã‚’é¸æŠã—ã¾ã™ã€‚" \
    "\n" \
    "8. **æ¤œç´¢çµæœã®è¡¨ç¤º**: æ¤œç´¢çµæœãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚å„è«–æ–‡ã®ã‚¿ã‚¤ãƒˆãƒ«ã€è‘—è€…ã€ç™ºè¡Œæ—¥ã€è¦ç´„ã€é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚" \
    "\n" \
    "9. **è‘—è€…ã§çµã‚Šè¾¼ã¿**: è‘—è€…åã§çµã‚Šè¾¼ã‚€ã“ã¨ãŒã§ãã¾ã™ã€‚" \
    "\n" \
    "10. **è¦ç´„ã®è¡¨ç¤º**: å„è«–æ–‡ã®è¦ç´„ã‚’è¡¨ç¤ºã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚")   

arxiv_keyword = st.text_input("arXivåˆ†é‡æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰(è‹±èª)")
query = st.text_input("æ—¥æœ¬èªã‚¯ã‚¨ãƒª")
use_morph = st.checkbox("æ—¥æœ¬èªã‚¯ã‚¨ãƒªã‚’å½¢æ…‹ç´ è§£æã§åˆ†å‰²ã™ã‚‹", value=True)

col1, col2 = st.columns(2)
with col1:
    date_from = st.date_input("ğŸ“… å–å¾—é–‹å§‹æ—¥", value=datetime.date.today() - datetime.timedelta(days=30))
with col2:
    date_to = st.date_input("ğŸ“… å–å¾—çµ‚äº†æ—¥", value=datetime.date.today())

sort_order = st.radio("ç™ºè¡Œæ—¥ã®ä¸¦ã³é †ã‚’é¸æŠã—ã¦ãã ã•ã„", options=["æ–°ã—ã„é †ï¼ˆé™é †ï¼‰", "å¤ã„é †ï¼ˆæ˜‡é †ï¼‰"], index=0)
user_wants_desc = "é™é †" in sort_order
sort_type = st.radio("ä¸¦ã³æ›¿ãˆã®åŸºæº–", options=["ã‚¹ã‚³ã‚¢é †", "ç™ºè¡Œæ—¥é †"], index=0)

if query and arxiv_keyword:
    if use_morph:
        keywords = extract_keywords(query)
        keywords.append(query)
    else:
        keywords = [query]
    st.write("æŠ½å‡ºã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰:", ", ".join(keywords))

    base = query
    candidates = get_wikipedia_related_words(base)
    if candidates:
        candidates = get_dynamic_related_words(base, candidates)

    selected = []
    if candidates:
        st.write("ğŸ’¡ é–¢é€£èªã®å€™è£œã‚’é¸ã‚“ã§ãã ã•ã„:")
        selected = [word for word in candidates if st.checkbox(word)]

    final_query = " ".join(keywords + selected)
    st.markdown(f"**æ‹¡å¼µå¾Œã‚¯ã‚¨ãƒª:** `{final_query}`")
    query_vec = model.encode(final_query, convert_to_numpy=True)

    with st.spinner("arXivã‹ã‚‰è«–æ–‡å–å¾—ä¸­..."):
        papers = fetch_arxiv_papers(
            query=arxiv_keyword,
            max_results=500,
            sort_order="descending",
            date_from=date_from,
            date_to=date_to
        )

        if not papers:
            st.warning("æŒ‡å®šã•ã‚ŒãŸæœŸé–“ã«è©²å½“ã™ã‚‹è«–æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        else:
            st.success(f"âœ… {len(papers)}ä»¶ã®è«–æ–‡ã‚’å–å¾—ã—ã¾ã—ãŸ")
            for p in papers:
                p["published_dt"] = datetime.datetime.strptime(p["published"], "%Y-%m-%d")

            selected_authors = st.multiselect("ğŸ‘¤ è‘—è€…ã§çµã‚Šè¾¼ã¿", sorted(set(a for p in papers for a in p["authors"])))
            if selected_authors:
                papers = [p for p in papers if any(a in selected_authors for a in p["authors"])]

            with st.spinner("ğŸ”„ ã‚¹ã‚³ã‚¢è¨ˆç®—ä¸­..."):
                paper_vecs = encode_papers(papers)
                scores = util.cos_sim(query_vec, paper_vecs)[0].cpu().numpy()
                papers = [p | {"score": scores[i]} for i, p in enumerate(papers) if scores[i] >= 0.2]

            if sort_type == "ã‚¹ã‚³ã‚¢é †":
                papers = sorted(papers, key=lambda x: x["score"], reverse=True)
            else:
                papers = sorted(papers, key=lambda x: x["published_dt"], reverse=user_wants_desc)

            results_per_page = 10
            total_results = len(papers)
            total_pages = (total_results - 1) // results_per_page + 1
            selected_page = st.number_input("ğŸ“„ è¡¨ç¤ºãƒšãƒ¼ã‚¸ã‚’é¸ã‚“ã§ãã ã•ã„", 1, total_pages, 1)
            start_idx = (selected_page - 1) * results_per_page
            end_idx = start_idx + results_per_page

            st.subheader(f"ğŸ“„ æ¤œç´¢çµæœï¼ˆãƒšãƒ¼ã‚¸ {selected_page} / {total_pages}ï¼‰")
            for idx, p in enumerate(papers[start_idx:end_idx], start=start_idx + 1):
                st.markdown(f"### {idx}. [{p['title']}]({p['link']})")
                st.write(f"**Authors**: {', '.join(p['authors'])}")
                st.write(f"**Published**: {p['published']}")
                st.write(f"**Similarity**: `{p['score']:.4f}`")
                st.progress(min(int(p['score'] * 100), 100))
                with st.expander("ğŸ“– è¦ç´„ã‚’è¡¨ç¤º"):
                    st.write(p["summary"])
