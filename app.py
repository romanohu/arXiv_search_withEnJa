import streamlit as st
import datetime
import numpy as np
from sentence_transformers import util

from utils import (
    get_wikipedia_related_words,
    extract_keywords,
    get_dynamic_related_words,
    fetch_arxiv_papers,
    encode_papers,
    get_model
)



model = get_model()

st.title("arXivè«–æ–‡æ¤œç´¢")
with st.expander("ä½¿ã„æ–¹"):
    st.write("ã“ã‚Œã¯è‹±èªã§ã—ã‹æ¤œç´¢ã§ããªã„arXivã«å¯¾ã—ã¦æ—¥æœ¬èªã§ã®æ¤œç´¢ã‚’è©¦ã¿ã‚‹æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã§ã™ï¼ä»¥ä¸‹ã®æ‰‹é †ã§ä½¿ç”¨ã§ãã¾ã™:" \
    "\n" \
    "1. **arXivåˆ†é‡æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰**: arXivã§èª¿ã¹ãŸã„è«–æ–‡ãŒè©²å½“ã™ã‚‹å¤§ã¾ã‹ãªåˆ†é‡ã‚’ã€Œè‹±èªã€ã§å…¥åŠ›ã—ã¾ã™ï¼ä¾‹:Machine Learning, LLMãªã©ï¼" \
    "\n" \
    "2. **æ—¥æœ¬èªã‚¯ã‚¨ãƒª**: ã‚ˆã‚Šè©³ç´°ãªæ¤œç´¢ã—ãŸã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚„ãƒ•ãƒ¬ãƒ¼ã‚ºã‚’ã€Œæ—¥æœ¬èªã€ã§å…¥åŠ›ã—ã¾ã™ï¼" \
    "\n" \
    "3. **å½¢æ…‹ç´ è§£æã®ä½¿ç”¨**: æ—¥æœ¬èªã‚¯ã‚¨ãƒªã‚’å½¢æ…‹ç´ è§£æã§åˆ†å‰²ã™ã‚‹ã‹ã©ã†ã‹ã‚’é¸æŠã—ã¾ã™ï¼" \
    "\n" \
    "4. **å–å¾—é–‹å§‹æ—¥ã¨çµ‚äº†æ—¥**: è«–æ–‡ã®å–å¾—æœŸé–“ã‚’æŒ‡å®šã—ã¾ã™ï¼" \
    "\n" \
    "5. **ç™ºè¡Œæ—¥ã®ä¸¦ã³é †**: è«–æ–‡ã®ç™ºè¡Œæ—¥ã®ä¸¦ã³é †ã‚’é¸æŠã—ã¾ã™ï¼" \
    "\n" \
    "6. **ä¸¦ã³æ›¿ãˆã®åŸºæº–**: æ¤œç´¢çµæœã®ä¸¦ã³æ›¿ãˆåŸºæº–ã‚’é¸æŠã—ã¾ã™ï¼" \
    "\n" \
    "7. **é–¢é€£èªã®é¸æŠ**: Wikipediaã‚„å½¢æ…‹ç´ è§£æã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸé–¢é€£èªã®å€™è£œã‹ã‚‰ï¼Œå¿…è¦ãªã‚‚ã®ã‚’é¸æŠã—ã¾ã™ï¼" \
    "\n" \
    "8. **æ¤œç´¢çµæœã®è¡¨ç¤º**: æ¤œç´¢çµæœãŒè¡¨ç¤ºã•ã‚Œã¾ã™ï¼å„è«–æ–‡ã®ã‚¿ã‚¤ãƒˆãƒ«ï¼Œè‘—è€…ï¼Œç™ºè¡Œæ—¥ï¼Œè¦ç´„ï¼Œé¡ä¼¼åº¦ã‚¹ã‚³ã‚¢ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ï¼" \
    "\n" \
    "9. **è‘—è€…ã§çµã‚Šè¾¼ã¿**: è‘—è€…åã§çµã‚Šè¾¼ã‚€ã“ã¨ãŒã§ãã¾ã™ï¼" \
    "\n" \
    "10. **è¦ç´„ã®è¡¨ç¤º**: å„è«–æ–‡ã®è¦ç´„ã‚’è¡¨ç¤ºã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼")   

arxiv_keyword = st.text_input("arXivåˆ†é‡æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰(è‹±èª)")
query = st.text_input("æ—¥æœ¬èªã‚¯ã‚¨ãƒª")
use_morph = st.checkbox("æ—¥æœ¬èªã‚¯ã‚¨ãƒªã‚’å½¢æ…‹ç´ è§£æã§åˆ†å‰²ã™ã‚‹", value=True)

col1, col2 = st.columns(2)
with col1:
    date_from = st.date_input("ğŸ“… å–å¾—é–‹å§‹æ—¥", value=datetime.date.today() - datetime.timedelta(days=30))
with col2:
    date_to = st.date_input("ğŸ“… å–å¾—çµ‚äº†æ—¥", value=datetime.date.today())

sort_type = st.radio("ä¸¦ã³æ›¿ãˆã®åŸºæº–", options=["ã‚¹ã‚³ã‚¢é †", "ç™ºè¡Œæ—¥é †"], index=0)
sort_order = st.radio("ä¸¦ã³é †ã‚’é¸æŠã—ã¦ãã ã•ã„", options=["æ–°ã—ã„é †ï¼ˆé™é †ï¼‰", "å¤ã„é †ï¼ˆæ˜‡é †ï¼‰"], index=0)

if sort_type == "ã‚¹ã‚³ã‚¢é †":
    # ã‚¹ã‚³ã‚¢é †ã®å ´åˆã€é™é †ãªã‚‰é«˜ã„é †ã€æ˜‡é †ãªã‚‰ä½ã„é †
    reverse_sort = "é™é †" in sort_order
    sort_key = "score"
else:
    # ç™ºè¡Œæ—¥é †ã®å ´åˆã€é™é †ãªã‚‰æ–°ã—ã„é †ã€æ˜‡é †ãªã‚‰å¤ã„é †
    reverse_sort = "é™é †" in sort_order
    sort_key = "published_dt"


if query and arxiv_keyword:
    if use_morph:
        keywords = extract_keywords(query)
        keywords.append(query)
    else:
        keywords = [query]
    st.write("æŠ½å‡ºã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰:", ", ".join(keywords))

    base = query
    candidates = get_wikipedia_related_words(base)
    if candidates:
        candidates = get_dynamic_related_words(base, candidates)

    selected = []
    if candidates:
        if candidates:
            st.write("ğŸ’¡ é–¢é€£èªã®å€™è£œã‚’é¸ã‚“ã§ãã ã•ã„:")
            cols = st.columns(2)
            for i, word in enumerate(candidates):
                if cols[i % 2].checkbox(word):
                    selected.append(word)

    final_query = " ".join(keywords + selected)
    st.markdown(f"**æ‹¡å¼µå¾Œã‚¯ã‚¨ãƒª:** `{final_query}`")
    query_vec = model.encode(final_query, convert_to_numpy=True)

    with st.spinner("arXivã‹ã‚‰è«–æ–‡å–å¾—ä¸­..."):
        papers = fetch_arxiv_papers(
            query=arxiv_keyword,
            max_results=100,
            date_from=date_from,
            date_to=date_to
        )

        if not papers:
            st.warning("æŒ‡å®šã•ã‚ŒãŸæœŸé–“ã«è©²å½“ã™ã‚‹è«–æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼")
        else:
            st.success(f"âœ… {len(papers)}ä»¶ã®è«–æ–‡ã‚’å–å¾—ã—ã¾ã—ãŸ")
            for p in papers:
                p["published_dt"] = datetime.datetime.strptime(p["published"], "%Y-%m-%d")

            selected_authors = st.multiselect("ğŸ‘¤ è‘—è€…ã§çµã‚Šè¾¼ã¿", sorted(set(a for p in papers for a in p["authors"])))
            if selected_authors:
                papers = [p for p in papers if any(a in selected_authors for a in p["authors"])]

            with st.spinner("ğŸ”„ ã‚¹ã‚³ã‚¢è¨ˆç®—ä¸­..."):
                paper_vecs = encode_papers(papers)
                scores = util.cos_sim(query_vec, paper_vecs)[0].cpu().numpy()
                papers = [p | {"score": scores[i]} for i, p in enumerate(papers)]

            if sort_key == "score":
                papers = sorted(papers, key=lambda x: x["score"], reverse=reverse_sort)
            else:
                papers = sorted(papers, key=lambda x: x["published_dt"], reverse=reverse_sort)

            results_per_page = 10
            total_results = len(papers)
            total_pages = (total_results - 1) // results_per_page + 1
            selected_page = st.number_input("ğŸ“„ è¡¨ç¤ºãƒšãƒ¼ã‚¸ã‚’é¸ã‚“ã§ãã ã•ã„", 1, total_pages, 1)
            start_idx = (selected_page - 1) * results_per_page
            end_idx = start_idx + results_per_page

            st.subheader(f"ğŸ“„ æ¤œç´¢çµæœï¼ˆãƒšãƒ¼ã‚¸ {selected_page} / {total_pages}ï¼‰")
            for idx, p in enumerate(papers[start_idx:end_idx], start=start_idx + 1):
                st.write(f"{idx}. [{p['title']}]({p['link']})")
                st.write(f"**Authors**: {', '.join(p['authors'])}")
                st.write(f"**Published**: {p['published']}")
                st.write(f"**Similarity**: `{p['score']:.4f}`")
                progress_value = min(max(int(p['score'] * 100), 0), 100)  # 0ï½100ã«ã‚¯ãƒªãƒƒãƒ—
                st.progress(progress_value)
                with st.expander("ğŸ“– è¦ç´„ã‚’è¡¨ç¤º"):
                    st.write(p["summary"])
